{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zip argument #1 must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b66e2e73cd40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtranspose_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-b66e2e73cd40>\u001b[0m in \u001b[0;36mtranspose_list\u001b[0;34m(mylist)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranspose_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtranspose_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
     ]
    }
   ],
   "source": [
    "def transpose_list(mylist):\n",
    "    return list(map(list, zip(*mylist)))\n",
    "\n",
    "\n",
    "transpose_list([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: Could not seed environment <MultiAgentEnv instance>\u001b[0m\n",
      "\u001b[33mWARN: Could not seed environment <MultiAgentEnv instance>\u001b[0m\n",
      "\u001b[33mWARN: Could not seed environment <MultiAgentEnv instance>\u001b[0m\n",
      "\u001b[33mWARN: Could not seed environment <MultiAgentEnv instance>\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Data/env/maddpg/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "episode: 12/16  75% ETA:  0:00:00 |\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\           | \r"
     ]
    }
   ],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "import envs\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "\n",
    "# keep training awake\n",
    "# from workspace_utils import keep_awake\n",
    "\n",
    "# for saving gif\n",
    "import imageio\n",
    "\n",
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "def pre_process(entity, batchsize):\n",
    "    processed_entity = []\n",
    "    for j in range(3):\n",
    "        list = []\n",
    "        for i in range(batchsize):\n",
    "            b = entity[i][j]\n",
    "            list.append(b)\n",
    "        c = torch.Tensor(list)\n",
    "        processed_entity.append(c)\n",
    "    return processed_entity\n",
    "\n",
    "\n",
    "\n",
    "seeding()\n",
    "# number of parallel agents\n",
    "parallel_envs = 4\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "number_of_episodes = 16\n",
    "episode_length = 10\n",
    "batchsize = 10\n",
    "# how many episodes to save policy and gif\n",
    "save_interval = 5000\n",
    "# what is this ?\n",
    "t = 0\n",
    "\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "noise = 2\n",
    "noise_reduction = 0.9999\n",
    "\n",
    "# how many episodes before update\n",
    "episode_per_update = 2 * parallel_envs\n",
    "\n",
    "log_path = os.getcwd()+\"/log\"\n",
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "torch.set_num_threads(parallel_envs)\n",
    "# this may be a list of all environments\n",
    "env = envs.make_parallel_env(parallel_envs)\n",
    "\n",
    "# keep 5000 episodes worth of replay\n",
    "buffer = ReplayBuffer(int(5000*episode_length))\n",
    "\n",
    "# initialize policy and critic\n",
    "# this creates a list of models, each element in the list refers to an agent in the simulation\n",
    "# [agent_one_ddpg, agent_two_ddpg, ...]\n",
    "# agent_one_ddpg contains the agent actor and critic models,e.g., agent_one_ddpg.actor, agent_one_ddpg.critic\n",
    "maddpg = MADDPG()\n",
    "logger = SummaryWriter(log_dir=log_path)\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "agent2_reward = []\n",
    "\n",
    "# training loop\n",
    "# show progressbar\n",
    "import progressbar as pb\n",
    "widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "          pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "\n",
    "# use keep_awake to keep workspace from disconnecting\n",
    "# for episode in keep_awake(range(0, number_of_episodes, parallel_envs)):\n",
    "# notice we jump forward by number of parallel environments\n",
    "for episode in range(0, number_of_episodes, parallel_envs):\n",
    "    timer.update(episode)\n",
    "\n",
    "    # i believe there are as many as number of agents times parallel env reward\n",
    "    reward_this_episode = np.zeros((parallel_envs, 3))\n",
    "    # obs is the observation state space of all the three agents in the 4 parallel env.\n",
    "    # for the Physical Dception environment with three agents it is of dimension 4x3x14.\n",
    "    # obs_full is world state irrespective of the agents and its dimension is 4x14.\n",
    "    all_obs = env.reset()\n",
    "    obs, obs_full = transpose_list(all_obs)\n",
    "\n",
    "\n",
    "   #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "    # save info or not\n",
    "    save_info = (episode % save_interval < parallel_envs or episode == number_of_episodes-parallel_envs)\n",
    "    frames = []\n",
    "    tmax = 0\n",
    "\n",
    "    if save_info:\n",
    "        frames.append(env.render('rgb_array'))\n",
    "\n",
    "\n",
    "\n",
    "    for episode_t in range(episode_length):\n",
    "        # t jumps forward in a multiple of environment\n",
    "        t += parallel_envs\n",
    "\n",
    "\n",
    "        # explore = only explore for a certain number of episodes\n",
    "        # action input needs to be transposed\n",
    "        # the transpose_to_tensor(obs) changes the data to each agent point of view\n",
    "        # since we have 4 environments, there are 4 agent 1, 4 agent 2, and 4 agent 3\n",
    "        # each agent has a state in each environment, total states across 4 environments for agent 1 is 4x14 tensor\n",
    "        # transpose_to_tensor(obs) = is a list of 3 elements. each element is for 1 agent\n",
    "        # pick element 1. this is an array of 4x14 elements of agent observation across 4 environments.\n",
    "        # maddpg.act has a for loop that take each element of obs and pass it to the agents actor models and \n",
    "        # to generate an action from each agent actor. \n",
    "        actions = maddpg.act(transpose_to_tensor(obs), noise=noise)\n",
    "        noise *= noise_reduction\n",
    "        # there are 4 actions per agent and 3 agents, total of 12 actions \n",
    "        actions_array = torch.stack(actions).detach().numpy()\n",
    "        \n",
    "        actions_for_env = np.rollaxis(actions_array, 1)\n",
    "        \n",
    "        \n",
    "        # step forward one frame\n",
    "        # obs is the observation state space of all the three agents in the 4 parallel env.\n",
    "        # for the Physical Dception environment with three agents it is of dimension 4x3x14.\n",
    "        # obs_full is world state irrespective of the agents and its dimension is 4x14.\n",
    "        # To gain more understanding, please see the code in the multiagent folder.\n",
    "        next_obs, next_obs_full, rewards, dones, info = env.step(actions_for_env)\n",
    "\n",
    "#         # add data to buffer\n",
    "#         transition = (obs, obs_full, actions_for_env, rewards, next_obs, next_obs_full, dones)\n",
    "\n",
    "#         buffer.push(transition)\n",
    "\n",
    "#         reward_this_episode += rewards\n",
    "\n",
    "#         obs, obs_full = next_obs, next_obs_full\n",
    "\n",
    "\n",
    "\n",
    "#     samples = buffer.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.07381478,  0.22205014,  0.22205014],\n",
       "       [-0.20003734,  0.37448418,  0.37448418],\n",
       "       [-0.57595087,  0.57313365,  0.57313365],\n",
       "       [-1.64609881,  0.46162903,  0.46162903]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, obs_full, action, reward, next_obs, next_obs_full, done = map(transpose_to_tensor, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_obs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_obs[0,:][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85010052, -0.04933641, -0.34143099, -0.76787591,  0.        ,\n",
       "        0.        , -0.45229252,  0.21949375,  0.        ,  0.        ,\n",
       "        0.08854489, -1.27879257,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_obs[0,:][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_full[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transpose_to_tensor(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 14])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transpose_to_tensor(obs)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_for_env.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
